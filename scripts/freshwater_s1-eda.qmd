---
title: "Freshwater Water Quality Exploratory Data Analysis"
subtitle: "A Healthy Waters Partnership Analysis"
description: "Script 1 in a series of script designed to analyse, score, and present freshwater water quality in the Dry Tropics region. The output of this is used in the Dry Tropics Technical Report."
author: "Adam Shand"
format: html
params:
  script_crs: "EPSG:7844"
  script_fyear: 2024
---

```{r}

#load in a custom function to get yaml parameters
source("functions/get_quarto_params.R")

params <- get_quarto_params()

#return each param as its own object
script_crs <- params$script_crs
script_fyear <- params$script_fyear

```

# Introduction

The purpose of this script is to prepare and perform EDA for the freshwater water quality data for the dry tropics technical report. Key steps include:

 - Loading all metadata
 - Loading all sampling data
 - Assigning metadata to every sample
 - Performing QA/QC such as checking LOR values, conducting EDA, calculating summary statistics
 - Creating Histograms
 - Creating box plots
 - Creating line plots

# Script Set Up

This script requires multiple core spatial packages, each of which is loaded below. Key variables and paths are also created.

```{r}
#| label: load packages

#use pacman function to load and install (if required) all other packages
pacman::p_load(tidyverse, glue, here, sf, tmap, janitor, readxl, ggplot2, viridis, busdater, stars)

#install/load the custom RcTools package
#pak::pak("add-am/RcTools")
library(RcTools)

#load in the custom function used to create the read and write folders for the script
source("functions/setup_helper.R")

#get the required paths
paths <- setup_helper(script_fyear)
data_path <- paths[[1]]
output_path <- paths[[2]]

#turn off scientific notation
options(scipen = 999)
sf_use_s2(FALSE)

```

# Load Data

Data for this script is provided in two spreadsheets: a metadata spreadsheet, and a master spreadsheet. 

The metadata spreadsheet contains (for all current sites):

 - Grouping information for each site (region, environment, basin, sub_basin, watercourse, code),
 - Location information for each site i.e., where the site is (lat, long), and how big is the area it is in,
 - Supplier information (who provides the data),
 - Alternative names the site might be known by,
 - Water quality objective (WQO) information for each site,
 - Limit of reporting (LOR) information for each site,
 - Scaling factor (SF) information for each site,
 - The original source for WQO, LOR, and SF values,
 - plus the above information for all historical sites.
 
The master spreadsheet contains:

 - Codes for all data, to be cross checked against metadata,
 - All sample data for all current sites for as far back as possible (~2019),
 - All currently monitored variables (DIN, TP, Turbidity, DO, and soon to be FRP),
 - Units for each variable, to cross check against metadata,
 - Peripheral variables as need to calculated main variables (e.g., DIN = Ammonia + NOx, or Temp to calculate %DO saturation).

## Metadata Spreadsheet

```{r}
#| label: load metadata

#create list of sheets to target
target_sheets <- c("Current_Sites", "WQO", "LOR", "SF")

#create empty dataframe to hold metadata
site_metadata <- data.frame()

#get the total number of sheets excluding the first one (which is the README sheet)
for (i in 1:length(target_sheets)){
  
  #read in the target sheet
  temp_sheet <- read_excel(glue("{data_path}/dt_wq_freshwater_metadata.xlsx"),
                           sheet = target_sheets[i], na = c("", "NA", "NULL", "null")) 

  #if it is the first sheet(the current sites sheet) then rename it to the main metadata variable
  if (i == 1){site_metadata <- temp_sheet |> select(-c(SiteName, AKA))} 
  
  else {#otherwise, pivot the data and then merge it onto the main metadata variable
    
    temp_sheet <- pivot_longer(temp_sheet, cols = 2:ncol(temp_sheet), #select data, then target columns
                               names_to = c(".value", "Indicator"), names_pattern = "([A-Za-z]+)_(.+)")
    #.value indicates this part of the col name defines the name of the output column (it overrides the "values_to" arg)
    #names_pattern = how to split. this has two groups marked by brackets: 1. any letter left of underscore, 2. everything else.
    
    #merge onto main
    site_metadata <- merge(site_metadata, temp_sheet)}

}

#add units of measure, direction of failure, and statistic used. These should be able to be crossed check against the human readable pages in the excel metadata spreadsheet if anything seems off
site_metadata <- site_metadata |> 
  mutate(Units = case_when(Indicator == "Turbidity" ~ "NTU", 
                           Indicator %in% c("High_DO", "Low_DO") ~ "%.Sat",
                           T ~ "mg.L"), 
         Failure = case_when(Indicator == "Low_DO" ~ "low", T ~ "high"), Stat = "median")

#run the custom naming cleaning function to ensure everything is formatted as it should be
site_metadata <- name_cleaning(site_metadata)

#read in the historical sites and just keep the site codes, this is all that is needed to  filter the data later
historical_metadata <- read_excel(
  glue("{data_path}/dt_wq_freshwater_metadata.xlsx"),
  sheet = "Historic_Sites", na = c("", "NA", "NULL", "null")) |> 
    pull(Code)

#clean up
rm(target_sheets, temp_sheet)

```

## Master Data Spreadsheet

The master data sheet is built over a couple of stages:

### Import Datasets

Each of the datasets needs to be imported, they are stored in separate sheets so must be read in separately. 

 !!! need to change this, now do manually - Note the logic to remove the < symbol. These values are handled later using the metadata information.
 - Note the replacement of "aturat" (catches any variation of "saturated" (caps of otherwise)) with the value 100.
 - Note dates: some are provided as date_time, others, only date. The edit drops time and does not affect rows without time attached.

```{r}
#| label: Import datasheets

#get a variable listing all available sheets
sheets <- excel_sheets(glue("{data_path}/dt_wq_freshwater_data_master.xlsx"))

#get the total number of sheets excluding the first one (which is the README sheet)
for (i in 2:length(sheets)){
  
  #read in the target sheet
  temp_sheet <- read_excel(glue("{data_path}/dt_wq_freshwater_data_master.xlsx"), 
                           sheet = sheets[i], na = c("", "NA", "NULL", "null")) |> 
    select(-SiteName) #drop site name immediately as it is an uncontrolled variable only included for the human reader
  
  #assign the correct name, remove < symbols, and convert columns to numeric
  assign(sheets[i], temp_sheet |>
           mutate(across(contains("DO"), ~ str_replace(.x, ".*aturat.*", "100")),
                  across(c(3:ncol(temp_sheet)), as.numeric),
                  Date = Date + seconds(1), #add one second to avoid date-time error
                  Date = ymd_hms(Date), Fy = get_fy(Date), .after = Date) |> 
           filter(Fy > 2012,
                  Fy <= script_fyear)) #to allow for specific reporting year targeting
}

#clean up
rm(sheets, temp_sheet, i)

```

### Special Treatment of Datasets

Some of the datasets have unique treatments that they received before they can be combined together, for example, below we calculate the saturation of Dissolved Oxygen from the mg/L of Dissolved Oxygen. In this current script, this is the only special treatment that is required.

```{r}
#| label: calculate dissolved oxygen

#The TCC Other datasheet "special" process is converting DO mg/L to DO % saturation, read in a conversion table
conversion_table_DO <- read_csv(glue("{data_path}/conversion_table_dissolved_oxygen.csv"))

#fit a linear model to the data
DO_lm <- lm(formula = DOSolubilityAtSaturation ~ poly(Temperature, 3), data = conversion_table_DO)

#divided the recorded DO_mg/L by the predicted max DO saturation at the recorded temperature and times by 100. As an example, according to our linear model, at 20C, for DO to be 100% saturated, the concentration must be 9.218633mg/L. Thus if our recorded DO is 9.218633mg/L we can convert that to 100% saturated. If our DO is less that 9.218633, it calculates as less than 100% saturated, and similarly for if recorded DO is greater than 9.218633.
TCC_Other <- TCC_Other |> 
  mutate(`DO_%.Sat` = (DO_mg.L/predict(DO_lm, data.frame(Temperature = Temperature_C), type = "response")) * 100) |> 
  select(-c(Temperature_C,DO_mg.L))

#clean up
rm(conversion_table_DO, DO_lm)

```

### Bind Datasheets

now all sheets can be combined and provided with metadata and the finishing touches added. This includes adjusting values that are at or below the limit of reporting.

```{r}
#| label: bind datasheets

#Each of the spreadsheets (now aligned) can be given the same process
freshwater_wq <- bind_rows(DES, TCC_Wastewater, TCC_Other, CLMP) 

#create extra indicator columns
freshwater_wq_all <- freshwater_wq |> rowwise() |> 
  mutate(DIN_mg.L = sum(Ammonia_mg.L, NOX_mg.L, na.rm = T),
         DIN_mg.L = case_when(DIN_mg.L == 0 ~ NA, T ~ DIN_mg.L),
         "Low_DO_%.Sat" = `DO_%.Sat`) |> #calculate DIN, and create a low DO column
  rename("High_DO_%.Sat" = `DO_%.Sat`) |> #create a high DO column
  ungroup()

#pivot data around indicators and units, remove values that are NA
freshwater_wq_all <- freshwater_wq_all |> 
  pivot_longer(cols = 4:ncol(freshwater_wq_all), names_to = "Indicator", values_to = "Values") |> #pivot data longer
  separate_wider_regex("Indicator", c("Indicator" = ".*", "_", "Units" = ".*")) |> #split names into indicator and units
  group_by(Code, Date, Fy, Indicator, Units) |> 
  summarise(Values = mean(Values, na.rm = T)) |> ungroup() |>  #get daily mean
  left_join(site_metadata) |> 
  filter(!is.na(Values))

#run the custom naming cleaning function to ensure everything is formatted as it should be
freshwater_wq_all <- name_cleaning(freshwater_wq_all)

#clean up
rm(DES, TCC_Wastewater, TCC_Other, CLMP, freshwater_wq)

```

### Drop Historical Sites

Below a simple vector is used to filter out any sites that were listed as historic (i.e. no longer used).

```{r}

#only keep sites that don't exist in the historical sites list
freshwater_wq_all <- filter(freshwater_wq_all, !Code %in% historical_metadata)

```

# QA/QC Checks

We can now take this opportunity to perform the required QA/QC Checks, at the moment this includes:

 - Units
 - Locations and Codes
 - LOR/value comparison
 - LOR/WQO comparison
 
Please add more as you see fit.

## Unit Check

First we will check the units, we are looking to detect if any of the indicators have more than one set of units assigned.

```{r}
#| label: Unit Check
#| warning: true

#group by indicator and units
unit_check <- freshwater_wq_all |> 
  group_by(Indicator, Units) |> 
  summarize(.groups = "drop")

#then check if the number of indicators is equal to the number of indicator-unit pairs
if (length(unique(unit_check$Indicator)) == n_distinct(unit_check)){
  
  print("Test passed. No erroneous indicator units.")
  
  rm(unit_check)
  
} else {
  
  #save csv
  write_csv(unit_check, glue("{output_path}/spreadsheets/unit_check.csv"))

stop("Test fail, an indicator has data with two different units. Raw data might be provided with two 
     different units, or the units listed in the metadata may not match the master data. Review the
     'unit_check' table for more information.")
}

```

## Locations and Codes

We will check the number of locations found in the dataset matches what we expect to find (which for freshwater is 16 unique watercourses and 24 unique codes (sites)).

```{r}
#| label: Location and Code Check
#| warning: true

#group by the different levels of location
location_check <- freshwater_wq_all |>
  filter(Fy == script_fyear) |> #make sure we are only checking the current year of data
  group_by(Region, Environment, BasinOrZone, SubBasinOrSubZone, WatercourseOrGeographicArea) |> 
  summarize(.groups = "drop")

#then check if we have the right number of locations
if (nrow(location_check) == 16 & !any(is.na(location_check))){
  
  print("Test passed. Correct number of locations.")
  
  rm(location_check)
  
} else {
  
  #save csv
  write_csv(location_check, glue("{output_path}/spreadsheets/location_check.csv"))

stop("Test fail, currently the DT report should have 16 unique watercourses, please cross check if 
     there are locations missing, or if there have been new locations added. Review the 
     'location_check' table for more information.")
}

#group by watercourse and code
code_check <- freshwater_wq_all |> 
  filter(Fy == script_fyear) |> #make sure we are only checking the current year of data
  group_by(WatercourseOrGeographicArea, Code) |> 
  summarize(.groups = "drop")

#then check if we have the right number of watercourses and codes
if (nrow(code_check) == 24 & !any(is.na(code_check))){
  
  print("Test passed. Correct number of sites.")
  
  rm(code_check)
  
} else {
  
  #save csv
  write_csv(code_check, glue("{output_path}/spreadsheets/code_check.csv"))

stop("Test fail, currently the DT report should have 24 unique codes(sites), please cross check if 
     there are codes missing, or if there have been new codes added. Review the 'code_check' table
     for more information.")
}

```

## LOR/Value Comparison

We will now compare the LOR values against the concentration values recorded in the master spreadsheet. What we are trying to spot is how many of the concentrations values are half (or less) of the LOR. If lots of the concentration values are half the LOR then this is a warning sign that the LOR is likely not low enough and not fit for purpose.

```{r}
#| label: LOR Value check
#| warning: true

#check how many values are half of the LOR compared to the total number of values for the reporting period
value_lor_check <- freshwater_wq_all |> 
  filter(Fy == script_fyear) |> 
  mutate(`ValueHalved?` = case_when(Values <= Lor ~ "ValueHalved", 
                                     T ~ "ValueUnchanged")) |> 
  group_by(Code, Indicator, `ValueHalved?`) |> 
  summarise(Count = n(), .groups = "drop") |> 
  pivot_wider(names_from = `ValueHalved?`, values_from = Count) |> 
  filter(!is.na(ValueHalved))

if (!any(value_lor_check$ValueHalved > value_lor_check$ValueUnchanged)){
  
  print("Minimal values were changed.")
  
  rm(value_lor_check)
  
} else {
  
  #save the value lor check table
  write_csv(value_lor_check, glue("{output_path}/spreadsheets/value_lor_check.csv"))

warning("At >= one site, there are more values that have been changed to half the LOR than have 
been left unchanged. This is not inherently wrong, but should be double checked. Check the 
'value_lor_check' table for a list of all sites with changed values.")

}

```

## LOR/WQO Comparison

Finally we will compare the LOR and WQO values. This an important comparison as if the WQO value is equal to or less than the LOR value it becomes impossible to determine if the actual concentration values are equal to or less than the WQO value. For example:

 - If LOR = 1,
 - And WQO = 0.5,
 - The lowest a concentration can be recorded is <1 (half LOR).
 - Therefore, is the concentration (<1) lower than the WQO (0.5)?
 - It is impossible to know.
 
The below code chunk will flag any site that has:
 - an LOR <= WQO and,
 - more than half of the concentration values also <= WQO (i.e. cannot be interpreted). 
 
For any site that is flagged, the offending indicator will be removed.

```{r}
#| label: LOR WQO check
#| warning: true

#check if the LOR is equal to or greater than the WQO, whilst also checking if any values are also in this range
wqo_lor_check_p1 <- freshwater_wq_all |> 
  filter(Fy == script_fyear, Indicator != Lor) |> #this gets current year and ignore FRP
  mutate(Comparison = case_when(Lor >= Wqo & Values > Lor ~ "Val_Warn",
                                Lor >= Wqo & Values <= Lor ~ "Val_Fail",
                                T ~ "Val_Pass")) |>
  group_by(Code, Indicator, Lor, Wqo, Comparison) |> 
  summarise(Count = n(), .groups = "drop") |>
  pivot_wider(names_from = Comparison, values_from = Count)

#clean up the main dataset by removing ammonia and NOX as they are no longer needed after this check has been completed.
freshwater_wq_all <- freshwater_wq_all |> filter(!Indicator %in% c("Ammonia", "NOX"))

#check if warning and fail values exists
if (any("Val_Fail" %in% colnames(wqo_lor_check_p1))){

  #conduct checks for each indicator to determine if enough samples are above LOR/WQO to be allowed to use the site
  wqo_lor_check_p2 <- wqo_lor_check_p1 |> filter(Indicator != "DIN") |> 
    mutate(across(everything(), ~ replace_na(.x, 0)),
           Indicator = case_when(Indicator %in% c("Ammonia", "NOX") ~ "DIN", 
                                 T ~ Indicator)) |> 
    group_by(Code, Indicator) |> 
    summarise(across(contains("Val"), ~ sum(.x)),
              Keep_Site = case_when(Val_Fail < (if(exists("wqo_lor_check_p1$Val_Warn")){Val_Warn + Val_Pass} else {Val_Pass}) ~ "Keep", 
                                    T ~ "Remove"), .groups = "drop") |> 
    ungroup()
  
  #save the lor WQO check table
  write_csv(wqo_lor_check_p2, glue("{output_path}/spreadsheets/wqo_lor_check.csv"))
  
  #check if any sites need to be removed
  
  if (!any(str_detect(wqo_lor_check_p2$Keep_Site, "Rem"))){
    
    #if no sites need to be removed save the data
    write_csv(freshwater_wq_all, glue("{output_path}/spreadsheets/freshwater_wq_all.csv"))
  
    print("Test passed. No sites need to be removed.")
    
  } else { #if warning and fail values outnumber values that pass then remove those rows
  
    #to remove problem sites create a table containing the code-indicator pair of any failed sites
    removal_table <- wqo_lor_check_p2 |> 
      filter(Keep_Site == "Remove") |> 
      select(Code, Indicator)
  
    #use the full dataset to add all information about the code-indicator pair of any failed sites
    removal_table <- semi_join(filter(freshwater_wq_all, Fy == script_fyear), removal_table, by = join_by(Code, Indicator))
  
    #subtract from the main table, any rows that appear in the removal table
    freshwater_wq_removed <- suppressMessages(anti_join(freshwater_wq_all, removal_table))
      
    #save the before and after site removal tables
    write_csv(freshwater_wq_all, glue("{output_path}/spreadsheets/freshwater_wq_all_pre_site_removal.csv"))
    write_csv(freshwater_wq_removed, glue("{output_path}/spreadsheets/freshwater_wq_all_post_site_removal.csv"))
    
    #save the table that lists what was actually removed
    write_csv(removal_table, glue("{output_path}/spreadsheets/freshwater_wq_all_sites_removed.csv"))
    
    warning("At >= one site, the LOR is >= WQO AND >= 50% of the values are <= LOR. This means at 
    least one site has failed. The offending sites and indicators have been removed, however the 
    code will provide a warning here so the user is aware and can inspect the cause of the issue.")

  }
  
} else {#if there are no fail values
  
  #save the dataset
  write_csv(freshwater_wq_all, glue("{output_path}/spreadsheets/freshwater_wq_all.csv"))
  
  print("Test passed. No sites need to be removed.")

}    

```

The code chunk above has automatically removed the offending indicators from the offending sites. Now, in the code chunk below, we will overwrite the main freshwater dataset with the dataset we just created that has some values removed.

```{r}
#| label: overwrite old data

#if data needed to be removed, store the main dataset as "old" and assign the updated dataset back to the main dataset
if (exists("freshwater_wq_removed")){
  
  freshwater_wq_pre_removal <- freshwater_wq_all
  
  freshwater_wq_all <- freshwater_wq_removed}

```

# EDA Checks

After some mostly automated QA/QC we will now conduct some exploratory data analysis to see if we can spot anything wrong with this years data.

```{r}
#| label: select this years data

freshwater_wq_cy <- freshwater_wq_all |> filter(Fy == script_fyear)

```

## Histograms

Next we will create histograms for this years data, e.g.:

```{r}
#| label: EDA histograms

for (i in unique(freshwater_wq_cy$Indicator)){#for each indicator
  
  #get only that indicator
  target_data <- freshwater_wq_cy |> filter(Indicator == i)
  
  if (str_detect(i, "DO")){#if the indicator is either High DO or Low DO, rename to just DO
    target_data <- target_data |> mutate(Indicator == "DO")
    i <- "DO"
  }
  
  #plot data
  temp_plot <- ggplot(target_data, aes(x = Values, color = Code, fill = Code)) +
    geom_histogram(alpha = 0.6, bins = 100) +
    viridis::scale_fill_viridis(discrete = T) +
    viridis::scale_colour_viridis(discrete = T) +
    theme_bw() +
    theme(legend.position = "none",
          panel.spacing = unit(0.1, "lines"),
          strip.text.x = element_text(size = 8)) +
      xlab(glue("{i}")) +
      ylab("Count") +
    facet_wrap(~Code, scales = "free")
  
  #save to file
  ggsave(glue("{output_path}/plots/{i}_histogram.png"))
  
}

#show last plot
temp_plot

```

## Boxplots

and as a box plots, e.g.:

```{r}
#| label: EDA Boxplots

#create custom log function for tick breaks
base_breaks <- function(n = 10){
    function(x) {
        axisTicks(log10(range(x, na.rm = TRUE)), log = TRUE, n = n)
    }
}

for (i in unique(freshwater_wq_cy$Indicator)){#for each indicator
  
  #get only that indicator
  target_data <- freshwater_wq_cy |> filter(Indicator == i)
  
  if (str_detect(i, "DO")){#if the indicator is either High DO or Low DO, rename to just DO
    target_data <- target_data |> mutate(Indicator == "DO")
    i <- "DO"
  }
  
  #plot data
  temp_plot <- ggplot(target_data, aes(x = Indicator, y = Values, fill = Code)) +
    geom_boxplot(alpha = 0.6) +
    theme_bw() +
    theme(legend.position = "none",
          panel.spacing = unit(0.1, "lines"),
          strip.text.x = element_text(size = 8)) +
    xlab("") +
    scale_y_continuous(trans = scales::log_trans(), breaks = base_breaks(),
                   labels = prettyNum) +
    facet_wrap(~Code, scales = "free")
  
  #save to file
  ggsave(glue("{output_path}/plots/{i}_boxplot.png"))
  
}

#show last plot
temp_plot

```

## Lineplots

and some line plots, e.g.:

```{r}
#| label: EDA line plots

for (i in unique(freshwater_wq_cy$Indicator)){#for each indicator
  
  #get only that indicator and convert from ymd_hms to ymd only
  target_data <- freshwater_wq_cy |> filter(Indicator == i) |> mutate(Date = ymd(substr(Date, 1, 10)))
  
  if (str_detect(i, "DO")){#if the indicator is either High DO or Low DO, rename to just DO
    target_data <- target_data |> mutate(Indicator == "DO")
    i <- "DO"
  }
  
  #plot data
  temp_plot <- ggplot(target_data, aes(x = Date, y = Values, color = Code)) +
    geom_line() +
    geom_point(size = 0.4) +
    theme_bw() +
    theme(legend.position = "none",
          panel.spacing = unit(0.1, "lines"),
          strip.text.x = element_text(size = 8)) +
    xlab(glue("{i}")) +
    scale_x_date(breaks = scales::pretty_breaks(n = 2))+
    facet_wrap(~Code, scales = "free")
  
  #save to file
  ggsave(glue("{output_path}/plots/{i}_lineplot.png"))
  
}

#show last plot
temp_plot

```

## Rainfall

We can then look at the timing of rainfall and when the samples were taken. (Note that to obtain rainfall data we have to take a slight detour).

A custom function builds (or reloads) the n3_region dataset.

``` {r}
#| label: Load data

#if the data is on file, load it, otherwise built it and save it for next time
if (file.exists(here("data/n3_region.gpkg"))){
  n3_region <- st_read(here("data/n3_region.gpkg"))
} else {
  n3_region <- build_n3_region()
  st_write(n3_region, here("data/n3_region.gpkg"))
}

```

Then the n3 region is used to extract (or reload) the rainfall data.

```{r}
#| label: rainfall eda

#read in the northern three data and cut down to freshwater dry tropics only
dry_tropics <- n3_region |> 
  filter(Environment != "Marine", Region == "Dry Tropics") |> 
  group_by(Region, BasinOrZone, SubBasinOrSubZone) |> 
  summarise(geom = st_union(geom)) |> 
  st_transform(script_crs)

if (!file.exists(glue("{data_path}/rain.nc"))){

  rain <- extract_rainfall(dry_tropics, "2012-01-31")
  write_mdim(rain, glue("{data_path}/rain.nc"))

} else {rain <- read_stars(glue("{data_path}/rain.nc"))}
  
#create a tibble of layer names and dates
name_date <- tibble("layer_name" = names(rain), "layer_date" = time(rain))

#get mean from all layers and convert into a table, clean column names and add metadata
monthly_sub_basin_rainfall <- rain |> 
  aggregate(dry_tropics, FUN = mean, na.rm = TRUE) |> 
  as.data.frame() |>  
  select(-geom) |> 
  rename("MonthlyMeanRainfall" = 2, "LayerDate" = time) |> 
  mutate(
    BasinOrZone = rep(dry_tropics$BasinOrZone, nrow(name_date)),
    SubBasinOrSubZone = rep(dry_tropics$SubBasinOrSubZone, nrow(name_date)),
    MonthlyMeanRainfall = round(MonthlyMeanRainfall, 0))

#edit some sub basin names to make them match
sb_rain <- monthly_sub_basin_rainfall |> 
  mutate(SubBasinOrSubZone = case_when(
    str_detect(SubBasinOrSubZone, "(Lower)") ~ "Lower Ross River",
    str_detect(SubBasinOrSubZone, "(Upper)") ~ "Upper Ross River",
    str_detect(SubBasinOrSubZone, "Paluma") ~ "Paluma Dam",
    TRUE ~ SubBasinOrSubZone)
  ) #|> 
  
sb_rain <- sb_rain |>mutate("MonthlyMeanRainfall" = units::drop_units(MonthlyMeanRainfall))

```

And then we can then create the plots.

```{r}
#| label: create rainfall plots

for (i in unique(freshwater_wq_all$Indicator)){#for each indicator
  for (j in unique(freshwater_wq_all$SubBasinOrSubZone)){#and each sub basin
    
    #filter for the data we want
    freshwater_targeted <- freshwater_wq_all |> 
      filter(Indicator == i, SubBasinOrSubZone == j)

    rainfall_targeted <- sb_rain |> 
      filter(SubBasinOrSubZone == j, LayerDate >= min(freshwater_targeted$Date))
    
    if (all(is.na(freshwater_targeted$Values))){#if all values are NA, skip the loop
      
    } else { #create the plot
    
      #work out the coefficent between each dataset
      coeff <- max(rainfall_targeted$MonthlyMeanRainfall, na.rm = T)/max(freshwater_targeted$Values, na.rm = T)
      
      #Define the palette without "Blue"
      original_palette <- rev(RColorBrewer::brewer.pal(9, "YlOrRd"))
      
      #Add "Blue" to the beginning of the palette
      custom_palette <- c("blue", original_palette)
      
      #Identify unique values in the 'category' variable
      unique_codes <- c("Rainfall", unique(freshwater_targeted$Code))
      
      #Create a named vector associating colors with unique values
      color_mapping <- setNames(custom_palette[1:length(unique_codes)], unique_codes)
      
      #plot data
      temp_plot <- ggplot() +
        geom_line(data = rainfall_targeted, aes(x = LayerDate, y = MonthlyMeanRainfall/coeff, color = "Rainfall")) +
        geom_line(data = freshwater_targeted, aes(x = Date, y = Values, color = Code)) +
        theme_bw() +
        theme(panel.spacing = unit(0.1, "lines"),
              strip.text.x = element_text(size = 8)) +
        scale_x_datetime(breaks = scales::pretty_breaks(n = length(unique(year(freshwater_targeted$Date))))) +
        scale_y_continuous(name = glue("{i} ({freshwater_targeted$Units})"), 
                           sec.axis = sec_axis(~.*coeff, name = "Rainfall (mm)")) +
        labs(color = "Legend") +
        scale_colour_manual(values = color_mapping)
      
      #save plot
      ggsave(glue("{output_path}/plots/{i}_vs_rainfall_{j}.png"), temp_plot)
      
    }
  }
}

temp_plot

```

# Action Outstanding EDA

Earlier in the script we conducted preliminary QA/QC and were also able to automated some of the common actions required. However after conducting preliminary EDA there is no easy way to automate the actions required by the findings of this. Thus this section is the custom response to the EDA findings that were determined for the Eeach (financial year) data.

**It is important to note that this section should manually be rewritten each year to specifically address the EDA finding of the year.**

2023:

To make sure we don't miss anything we will go indicator by indicator:

 - DIN: Nothing major to note.
    + In the Box plots almost all sites have outliers. However these outliers are caused by almost all data being at the LOR and only a few being above the LOR, rather than crazy high/low values.
    + The Bohle sites are the only ones that show notable DIN values, this makes sense as they are downstream of a significant urban nitrogen source.
    + Rainfall and peak values match up roughly equally.
    + A max DIN value of 9.6 was recorded at BOH22.3 which is also realistic. 
 - DO: Nothing major to note.
    + Although the highest value as 144 - this is not an unrealistic measure. Similarly, in the boxplots it looks like DO zeros out, but the min value is actually 23.3, note the Y axis.
    + Rainfall and peak values match up roughly equally.
 - FRP: Not currently used in the report.
    + No need to critically analyse.
 - TP: Nothing major to note.
    + Similar to DIN, only Bohle sites show elevated values. This tracks due to their location. They also appear to report similar highs and lows at similar times.
    + Rainfall and peak values match up roughly equally.
    + A maximum value of 6.3 is realistic.
 - Turbidity: Nothing major to note
    + A maximum value of 166 was recorded at BOH18.1 This is within an expect measure of NTU.
    + AltC7.0 has recorded significantly higher values than neighboring creeks however is likely influenced by surrounding landuse.
    + Rainfall and peak values match up roughly equally.
    
    
2024:

To make sure we don't miss anything we will go indicator by indicator:

 - DIN:
    + Nothing major to note
 - DO:  
    + A couple of very low values were noted in the BOH18.1 site (<30%) and A.Weir site (<25%)
    + Very high values noted in Black River (>150)
 - FRP: 
    + Strange "stepped" pattern visible in OC3.7 lineplot, repeated values?
 - TP:
    + Nothing major to note
 - Turbidity: 
    + Nothing major to note

```{r}
#| label: action EDA findings

print("This year, no EDA findings required action.")

```

# Save Data

Now that all QA/QC and EDA actions are completed we can save the processed data in the main data folder, ready for the main analysis script. This save step is quite important as we dont want to overwrite old dataset prematurely, thus we will include the year of data that was targeted for the eda checks.

Also, once again it should be noted that the dataset may have had some values removed during the LOR/WQO QA/QC step. If this is the case the dataset will contain an extra bit of information in the file name stating as such.

```{r}
#| label: save data to main data folder

#slightly edit the data path to save these files
data_path <- str_replace(data_path, "raw", "processed")

if (exists("freshwater_wq_removed")) {# a removed and non-removed set exists, save both
  
  #save the data with the removed sites
  write_csv(freshwater_wq_all, 
            glue("{data_path}/{script_fyear-1}-{script_fyear}_freshwater_wq_all_sites_removed.csv"))
  
  #save the original without the removed sites - this is useful for inspecting at a later date
  write_csv(freshwater_wq_pre_removal, 
            glue("{data_path}/{script_fyear-1}-{script_fyear}_freshwater_wq_all_pre_removal.csv"))

} else {#otherwise just save the one
  
  write_csv(freshwater_wq_all, 
            glue("{data_path}/{script_fyear-1}-{script_fyear}_freshwater_wq_all.csv"))

}

```

```{r}


```

